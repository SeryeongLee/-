{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이콘 - 운동동작분류 1등 분석 (21.03.22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pandas에서 옵션 보기 (600개 행과 열)\n",
    "pd.set_option(\"display.max_column\", 600)\n",
    "pd.set_option(\"display.max_row\", 600)\n",
    "# 경고메세지 무시\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 경로 지정\n",
    "path = \"H:/fitness/\"\n",
    "\n",
    "train = pd.read_csv(path + \"data/train_features.csv\")\n",
    "# train data -> acc,gy,time으로 분리\n",
    "train_acc, train_gy  = train.iloc[:, 2:5], train.iloc[:, 5:]\n",
    "# 0~599 / 50 ?\n",
    "train_time = train.time[:600]/50\n",
    "\n",
    "train_label = pd.read_csv(path + \"data/train_labels.csv\")\n",
    "train_y = train_label.label\n",
    "\n",
    "test = pd.read_csv(path + \"data/test_features.csv\")\n",
    "submission = pd.read_csv(path + \"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time rolling\n",
    "rotation은 wearable센서의 무작위한 착용 방향을 고려한 데이터 증강 방법입니다.\n",
    "\n",
    "permutation은 신호를 n segment로 나누어 순서를 랜덤하게 바꿔주는 방법입니다.\n",
    "Permutation (Perm) is a simple way to randomly perturb the\n",
    "temporal location of within-window events. To perturb the location\n",
    "of the data in a single window, we first slice the data into N samelength\n",
    "segments, with N ranging from 1 to 5, and randomly permute\n",
    "the segments to create a new window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data/blob/master/Example_DataAugmentation_TimeseriesData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from transforms3d.axangles import axangle2mat\n",
    "\n",
    "# time rolling\n",
    "def rolling(data):\n",
    "    # np.random.choice(): 임의표본추출\n",
    "    for j in np.random.choice(data.shape[0], int(data.shape[0]*2/3)):\n",
    "        data[j] = np.roll(data[j], np.random.choice(data.shape[1]), axis= 0)\n",
    "    return data\n",
    "\n",
    "# data aug methods\n",
    "def rotation(data):\n",
    "    axis = np.random.uniform(low=-1, high=1, size=data.shape[1])\n",
    "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
    "    return np.matmul(data , axangle2mat(axis,angle))\n",
    "\n",
    "def permutation(data, nPerm=4, mSL=10):\n",
    "    data_new = np.zeros(data.shape)\n",
    "    idx = np.random.permutation(nPerm)\n",
    "    bWhile = True\n",
    "    while bWhile == True:\n",
    "        segs = np.zeros(nPerm+1, dtype=int)\n",
    "        segs[1:-1] = np.sort(np.random.randint(mSL, data.shape[0]-mSL, nPerm-1))\n",
    "        segs[-1] = data.shape[0]\n",
    "        if np.min(segs[1:]-segs[0:-1]) > mSL:\n",
    "            bWhile = False\n",
    "    pp = 0\n",
    "    for ii in range(nPerm):\n",
    "        data_temp = data[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
    "        data_new[pp:pp+len(data_temp),:] = data_temp\n",
    "        pp += len(data_temp)\n",
    "    return(data_new)\n",
    "\n",
    "# per + rot\n",
    "def combine_aug(data, k, aug_P = 0):\n",
    "    data_ = data.copy()\n",
    "    if aug_P == 0:\n",
    "        if (k+1) % 2 == 0:\n",
    "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
    "                data_[600*i:600*(i+1)] = rotation(np.array(data_[600*i:600*(i+1)]))\n",
    "        if (k+1) % 2 == 1:\n",
    "            for i in np.random.choice(int(data.shape[0]/600), int(data.shape[0]/600*2/3)):\n",
    "                data_[600*i:600*(i+1)] = permutation(np.array(data_[600*i:600*(i+1)]))\n",
    "                \n",
    "    if aug_P != 0:\n",
    "        pass\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강한 것 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "f, axes = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "\n",
    "f.set_size_inches((40, 6))\n",
    "f.patch.set_facecolor(\"white\")\n",
    "\n",
    "axes[0].plot(train_acc[:600])\n",
    "axes[0].set_title(\"ORIGINAL\", fontsize = 20)\n",
    "axes[1].plot(rotation(train_acc[:600]))\n",
    "axes[1].set_title(\"ROTATION\", fontsize = 20)\n",
    "axes[2].plot(permutation(np.array(train_acc[:600])))\n",
    "axes[2].set_title(\"PERMUTATION\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 사용한 데이터 증강 method\n",
    "\n",
    "짝수 epoch에 rolling, permutation 결합\n",
    "홀수 epoch에 rolling, rotation 결합\n",
    "LB public score 기준\n",
    "\n",
    "rolling만 사용했을경우 0.51\n",
    "rolling + rotation combine한 경우 0.41\n",
    "최종적인 aug 방법을 사용한 경우 0.386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import atan, sqrt\n",
    "from scipy.integrate import cumtrapz\n",
    "\n",
    "def get_mag(data):\n",
    "    return (data.iloc[:, 0]**2) + (data.iloc[:, 1]**2) + (data.iloc[:, 2]**2)\n",
    "\n",
    "def get_mul(data):\n",
    "    return data.iloc[:, 0] * data.iloc[:, 1] * data.iloc[:, 2]\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def get_roll_pitch(data):\n",
    "    roll = (data.iloc[:,1]/(data.iloc[:,0]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
    "    pitch = (data.iloc[:,0]/(data.iloc[:,1]**2 + data.iloc[:,2]**2).apply(lambda x : sqrt(x))).apply(lambda x : atan(x))*180/np.pi\n",
    "    return pd.concat([roll, pitch], axis= 1)\n",
    "\n",
    "##########################################################################################################################\n",
    "\n",
    "def setting(data, data_, case = 0):\n",
    "    if case == 0:\n",
    "        for i in range(0, data.shape[0], 600):\n",
    "            data[i] = data_[i] - data_[i+599]\n",
    "    else:\n",
    "        for i in range(0, data.shape[0], 600):\n",
    "            data[i: i+5] = data_[i: i+5].values - data_[i+594:i+599].values\n",
    "    return data\n",
    "        \n",
    "def get_diff(data, case = 0):\n",
    "    if case == 0:\n",
    "        x_dif, y_dif, z_dif = data.iloc[:, 0].diff(), data.iloc[:, 1].diff(), data.iloc[:, 2].diff()\n",
    "    else:\n",
    "        x_dif, y_dif, z_dif = data.iloc[:, 0].diff(5), data.iloc[:, 1].diff(5), data.iloc[:, 2].diff(5)\n",
    "    return pd.concat([setting(x_dif, data.iloc[:, 0], case),\n",
    "                      setting(y_dif, data.iloc[:, 1], case),\n",
    "                      setting(z_dif, data.iloc[:, 2], case)], axis= 1)\n",
    "############################################################################################################################\n",
    "\n",
    "def get_cumtrapz(acc):\n",
    "    acc_x, acc_y, acc_z = [], [], []\n",
    "    ds_x, ds_y, ds_z = [], [], []\n",
    "    for i in range(int(acc.shape[0]/600)):\n",
    "        acc_x.append(pd.DataFrame(cumtrapz(acc.iloc[600*i:600*(i+1), 0], train_time, initial=0)))\n",
    "        acc_y.append(pd.DataFrame(cumtrapz(acc.iloc[600*i:600*(i+1), 1], train_time, initial=0)))\n",
    "        acc_z.append(pd.DataFrame(cumtrapz(acc.iloc[600*i:600*(i+1), 2], train_time, initial=0)))\n",
    "        ds_x.append(pd.DataFrame(cumtrapz(cumtrapz(acc.iloc[600*i:600*(i+1), 0], train_time, initial=0), train_time, initial=0)))\n",
    "        ds_y.append(pd.DataFrame(cumtrapz(cumtrapz(acc.iloc[600*i:600*(i+1), 1], train_time, initial=0), train_time, initial=0)))\n",
    "        ds_z.append(pd.DataFrame(cumtrapz(cumtrapz(acc.iloc[600*i:600*(i+1), 2], train_time, initial=0), train_time, initial=0)))\n",
    "    return (pd.concat([pd.concat(acc_x), pd.concat(acc_y), pd.concat(acc_z)], axis = 1).reset_index(drop=True),\n",
    "           pd.concat([pd.concat(ds_x), pd.concat(ds_y), pd.concat(ds_z)], axis= 1).reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(acc_data, gy_data, i, aug_P = 0):\n",
    "\n",
    "    aug_acc = combine_aug(acc_data, i, aug_P)\n",
    "    aug_gy = combine_aug(gy_data, i, aug_P)\n",
    "    \n",
    "    diff_acc = get_diff(aug_acc)\n",
    "    #diff_acc_5 = get_diff(aug_acc, 1)\n",
    "    \n",
    "    roll_pitch_acc = get_roll_pitch(aug_acc)\n",
    "    mag_acc, mul_acc = get_mag(aug_acc), get_mul(aug_acc)\n",
    "    mag_mul_acc = pd.concat([mag_acc, mul_acc], axis= 1)\n",
    "    #accvel, disp = get_cumtrapz(aug_acc)\n",
    "\n",
    "    diff_gy = get_diff(aug_gy)\n",
    "    #diff_gy_5 = get_diff(aug_gy, 1)\n",
    "    mag_gy, mul_gy = get_mag(aug_gy), get_mul(aug_gy)\n",
    "    mag_mul_gy = pd.concat([mag_gy, mul_gy], axis= 1)\n",
    "\n",
    "    return pd.concat([aug_acc, diff_acc, roll_pitch_acc, mag_mul_acc,\n",
    "                     aug_gy, diff_gy, mag_mul_gy], axis= 1)\n",
    "\n",
    "def test_dataset(acc_data, gy_data):\n",
    "    \n",
    "    diff_acc = get_diff(acc_data)\n",
    "    #diff_acc_5 = get_diff(acc_data, 1)\n",
    "    \n",
    "    roll_pitch_acc = get_roll_pitch(acc_data)\n",
    "    mag_acc, mul_acc = get_mag(acc_data), get_mul(acc_data)\n",
    "    mag_mul_acc = pd.concat([mag_acc, mul_acc], axis= 1)\n",
    "    #accvel, disp = get_cumtrapz(acc_data)\n",
    "\n",
    "    diff_gy = get_diff(gy_data)\n",
    "    #diff_gy_5 = get_diff(gy_data, 1)\n",
    "    mag_gy, mul_gy = get_mag(gy_data), get_mul(gy_data)\n",
    "    mag_mul_gy = pd.concat([mag_gy, mul_gy], axis= 1)\n",
    "\n",
    "    return pd.concat([acc_data, diff_acc, roll_pitch_acc, mag_mul_acc,\n",
    "                      gy_data, diff_gy, mag_mul_gy], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaler\n",
    "train data만 사용하여 scaler를 만들었습니다.\n",
    "standscaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_for_scaler = test_dataset(train_acc, train_gy) # train data만 사용\n",
    "scaler = StandardScaler().fit(np.array(data_for_scaler))\n",
    "\n",
    "data_for_scaler = np.array(data_for_scaler).reshape(-1, 600, data_for_scaler.shape[1])\n",
    "########################################################################################\n",
    "test_x = test_dataset(test.iloc[:, 2:5], test.iloc[:, 5:])\n",
    "\n",
    "test_X = scaler.transform(np.array(test_x)).reshape(-1, 600, test_x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gru layer + pooling layer + dense layer를 조합하여 선택\n",
    "적은 데이터에는 gru를 사용하는 것이 accuracy와 loss사이의 trade off를 잘 조절해주는 것(개인적 견해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 모델을 찾기 위해 파라미터를 수정해 가면서 모니터링한 결과 각각의 모델의 성능은 비슷하나 설명할 수 있는 부분이 살짝 다르다고 판단하여\n",
    "각 모델당 2번씩 seed를 다르게 부여하여, 총 8개 결과의 평균을 냈습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def First_model():\n",
    "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
    "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
    "    ap = L.AveragePooling1D()(gru1)\n",
    "    gru2 = L.GRU(150, return_sequences = True)(ap)\n",
    "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
    "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
    "    return keras.models.Model(inputs, dense)\n",
    "\n",
    "def Second_model():\n",
    "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
    "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
    "    mp = L.MaxPool1D()(gru1)\n",
    "    ap = L.AveragePooling1D()(gru1)\n",
    "    concat1 = L.Concatenate()([mp, ap])\n",
    "    gru2 = L.GRU(150, return_sequences = True, dropout = 0.2)(concat1)\n",
    "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
    "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
    "    return keras.models.Model(inputs, dense)\n",
    "\n",
    "def Third_model():\n",
    "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
    "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
    "    mp = L.MaxPool1D()(gru1)\n",
    "    ap = L.AveragePooling1D()(gru1)\n",
    "    concat1 = L.Concatenate()([mp, ap])\n",
    "    gru2 = L.GRU(256, return_sequences = True, dropout = 0.2)(concat1)\n",
    "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
    "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
    "    return keras.models.Model(inputs, dense)\n",
    "\n",
    "def Fourth_model():\n",
    "    inputs = L.Input(shape = (data_for_scaler.shape[1], data_for_scaler.shape[2]))\n",
    "    gru1 = L.GRU(256, return_sequences = True, dropout = 0.2)(inputs)\n",
    "    ap = L.AveragePooling1D()(gru1)\n",
    "    gru2 = L.GRU(150, return_sequences = True, dropout = 0.2)(ap)\n",
    "    GAP = L.GlobalAveragePooling1D()(gru2)\n",
    "    dense = L.Dense(61, activation = \"softmax\")(GAP)\n",
    "    return keras.models.Model(inputs, dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "\n",
    "def train_model(model_ = None, epochs = 40, first_rlr = 15, second_rlr = 30, r_seed = 10, aug_P = 0, seed_ = 1):\n",
    "    # first_rlr : 첫번째로 learning_rate이 감소\n",
    "    # second_rlr : 두번째로 learning_rate이 감소\n",
    "    # r_seed : StratifiedKFold seed\n",
    "    # seed_ : numpy/random seed\n",
    "    \n",
    "    result_model = []\n",
    "    cnt = 0\n",
    "    array_acc = np.array(train_acc).reshape(-1, 600, 3)\n",
    "    array_gy = np.array(train_gy).reshape(-1, 600, 3)\n",
    "    \n",
    "    random.seed(seed_)\n",
    "    tf.random.set_seed(21)\n",
    "\n",
    "    split = StratifiedKFold(n_splits=10, shuffle = True, random_state = r_seed)\n",
    "    for train_idx, valid_idx in split.split(data_for_scaler, train_y):\n",
    "        \n",
    "        train_Y, valid_Y = np.array(pd.get_dummies(train_y))[train_idx], np.array(pd.get_dummies(train_y))[valid_idx]\n",
    "\n",
    "        valid_ACC, valid_GY = array_acc[valid_idx].reshape(-1, 3), array_gy[valid_idx].reshape(-1, 3)\n",
    "        valid_x = test_dataset(pd.DataFrame(valid_ACC), pd.DataFrame(valid_GY))\n",
    "        valid_X = scaler.transform(np.array(valid_x)).reshape(-1, 600, valid_x.shape[1])\n",
    "\n",
    "        model = model_()\n",
    "        model.compile(optimizer=keras.optimizers.RMSprop(0.003),\n",
    "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        val_score = 0\n",
    "        seed_ += 1\n",
    "\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            np.random.seed(seed_*47 + i)\n",
    "            \n",
    "            train_ACC, train_GY = array_acc[train_idx].reshape(-1, 3), array_gy[train_idx].reshape(-1, 3)\n",
    "            train_x = train_dataset(pd.DataFrame(train_ACC), pd.DataFrame(train_GY), i, aug_P)\n",
    "            train_X = scaler.transform(np.array(train_x)).reshape(-1, 600, valid_x.shape[1])\n",
    "\n",
    "            train_X_ = train_X.copy()\n",
    "\n",
    "            train_X_ = rolling(train_X_)\n",
    "\n",
    "            hist = model.fit(train_X_, train_Y, epochs = 1, validation_data = (valid_X, valid_Y), verbose = 0)\n",
    "\n",
    "            train_accuracy = hist.history[\"accuracy\"]\n",
    "            new_val_score = accuracy_score(np.argmax(valid_Y, axis = 1), np.argmax(model.predict(valid_X), axis = 1))\n",
    "            val_loss = hist.history[\"val_loss\"]\n",
    "\n",
    "            if i == first_rlr:\n",
    "                model.compile(optimizer=keras.optimizers.RMSprop(0.003*0.2),\n",
    "                              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            if i == second_rlr:\n",
    "                model.compile(optimizer = keras.optimizers.RMSprop(0.003*0.2*0.4),\n",
    "                             loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            print(\"epoch {} - train_accuracy : {} - validation_loss : {} - validation_accuracy : {}\".format(i,\n",
    "                                                                                                            train_accuracy,\n",
    "                                                                                                            val_loss,\n",
    "                                                                                                            new_val_score,\n",
    "                                                                                                            ))\n",
    "\n",
    "            if i == 0:\n",
    "                val_loss_score = val_loss[0]\n",
    "        \n",
    "            if val_loss_score >= val_loss[0]:\n",
    "                val_loss_score = val_loss[0]\n",
    "                best_model = model\n",
    "                print(\"####best_val####\")\n",
    "                    \n",
    "            if new_val_score >= val_score:\n",
    "                val_score = new_val_score\n",
    "                best_model = model\n",
    "                print(\"####best_acc####\")\n",
    "        print(\"####################################################### cycle {} is done\".format(cnt))\n",
    "        result_model.append(best_model)\n",
    "        cnt+=1\n",
    "    return result_model\n",
    "\n",
    "\n",
    "def predict_(model):\n",
    "    result = []\n",
    "    for mod in model:\n",
    "        result.append(mod.predict(test_X))\n",
    "    predict = np.array(result).mean(axis = 0)\n",
    "    return predict\n",
    "\n",
    "def save_model(models, name = '1'):\n",
    "    cnt = 1\n",
    "    for model in models:\n",
    "        model.save(path + \"submission/last/weight/\" + name + '-{}.h5'.format(cnt))\n",
    "        cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_result = train_model(First_model, r_seed = 47, seed_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result = train_model(Second_model, r_seed = 47, seed_ = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_result = train_model(First_model, r_seed = 32, seed_ = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_result = train_model(Second_model, r_seed = 32, seed_ = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_result = train_model(Third_model, r_seed = 2020, seed_ = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_result = train_model(Third_model, r_seed = 2020, seed_ = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seventh_result = train_model(Fourth_model, r_seed = 2020, seed_ = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eighth_result = train_model(Fourth_model, r_seed = 2020, seed_ = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(first_result)\n",
    "submission.iloc[:, 1:] = predict_(first_result)\n",
    "submission1 = submission\n",
    "#submission1.to_csv(path + \"submission/last/1.csv\", index = False)\n",
    "\n",
    "#save_model(second_result, \"2\")\n",
    "submission.iloc[:, 1:] = predict_(second_result)\n",
    "submission2 = submission\n",
    "#submission2.to_csv(path + \"submission/last/2.csv\", index = False)\n",
    "\n",
    "#save_model(third_result, \"3\")\n",
    "submission.iloc[:, 1:] = predict_(third_result)\n",
    "submission3 = submission\n",
    "#submission3.to_csv(path + \"submission/last/3.csv\", index = False)\n",
    "\n",
    "#save_model(fourth_result, \"4\")\n",
    "submission.iloc[:, 1:] = predict_(fourth_result)\n",
    "submission4 = submission\n",
    "#submission4.to_csv(path + \"submission/last/4.csv\", index = False)\n",
    "\n",
    "#save_model(fifth_result, \"5\")\n",
    "submission.iloc[:, 1:] = predict_(fifth_result)\n",
    "submission5 = submission\n",
    "#submission5.to_csv(path + \"submission/last/5.csv\", index = False)\n",
    "\n",
    "#save_model(sixth_result, \"6\")\n",
    "submission.iloc[:, 1:] = predict_(sixth_result)\n",
    "submission6 = submission\n",
    "#submission6.to_csv(path + \"submission/last/6.csv\", index = False)\n",
    "\n",
    "#save_model(seventh_result, \"7\")\n",
    "submission.iloc[:, 1:] = predict_(seventh_result)\n",
    "submission7 = submission\n",
    "#submission7.to_csv(path + \"submission/last/7.csv\", index = False)\n",
    "\n",
    "#save_model(eighth_result, \"8\")\n",
    "submission.iloc[:, 1:] = predict_(eighth_result)\n",
    "submission8 = submission\n",
    "#submission8.to_csv(path + \"submission/last/8.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission1 = pd.read_csv(path + \"submission/last/1.csv\")\n",
    "#submission2 = pd.read_csv(path + \"submission/last/2.csv\")\n",
    "#submission3 = pd.read_csv(path + \"submission/last/3.csv\")\n",
    "#submission4 = pd.read_csv(path + \"submission/last/4.csv\")\n",
    "#submission5 = pd.read_csv(path + \"submission/last/5.csv\")\n",
    "#submission6 = pd.read_csv(path + \"submission/last/6.csv\")\n",
    "#submission7 = pd.read_csv(path + \"submission/last/7.csv\")\n",
    "#submission8 = pd.read_csv(path + \"submission/last/8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:, 1:] = (submission1.iloc[:, 1:]/8 + submission2.iloc[:, 1:]/8 +\n",
    "                         submission3.iloc[:, 1:]/8 + submission4.iloc[:, 1:]/8 +\n",
    "                         submission5.iloc[:, 1:]/8 + submission6.iloc[:, 1:]/8 +\n",
    "                         submission7.iloc[:, 1:]/8 + submission8.iloc[:, 1:]/8)\n",
    "#submission.to_csv(path + \"submission/last/95.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
